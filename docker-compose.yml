services:
  # Serviço Laravel
  laravel:
    build:
      context: .
      dockerfile: docker/laravel/Dockerfile
    container_name: laravel-mcp
    ports:
      - "8000:8000"
    volumes:
      - ./laravel:/var/www/html
      - laravel_vendor:/var/www/html/vendor
      - laravel_node_modules:/var/www/html/node_modules
    environment:
      - APP_ENV=production
      - APP_DEBUG=false
      - APP_KEY=base64:${APP_KEY:-}
      - DB_CONNECTION=sqlite
      - DB_DATABASE=/var/www/html/database/database.sqlite
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - mcp-network

  # Serviço Ollama
  ollama:
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile
      args:
        OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.2}
    container_name: ollama-mcp
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
    restart: unless-stopped
    networks:
      - mcp-network
    # Descomente as linhas abaixo se tiver GPU NVIDIA
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  ollama_data:
  laravel_vendor:
  laravel_node_modules:

networks:
  mcp-network:
    driver: bridge
